{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)  #double precision\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "from jax import jacfwd\n",
    "\n",
    "# Special Transform Functions\n",
    "from jax import grad, jit, vmap, pmap\n",
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "from jax._src.lax.utils import (\n",
    "    _argnum_weak_type,\n",
    "    _input_dtype,\n",
    "    standard_primitive,)\n",
    "from jax._src.lax import lax\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Test_prk_for_optimization as IRK4\n",
    "import Important_functions.Transformation_Functions as TFunctions\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "import Important_functions.Energy_Error as EE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " : using the initial matrix A and B from the source given below :\n",
    " : wiki link : https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#:~:text=is%5B13%5D-,0,1/6,-A%20slight%20variation\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "############################### Original ##########################################\n",
    "\"\"\"\n",
    "## Lobatto 3A and B fourth order\n",
    "\n",
    "A1 = A2 = jnp.array([\n",
    "     [0., 0., 0., 0.],\n",
    "     [5/24, 1/3, -1/24, 0.],\n",
    "     [1/6, 2/3, 1/6, 0.],\n",
    "     [0., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "\n",
    "\n",
    "## Making the Halton code\n",
    "\n",
    "spacedim = [(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5) ]\n",
    "\n",
    "space = Space(spacedim)\n",
    "\n",
    "halton = Halton()\n",
    "n = 150\n",
    "\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "\n",
    "## Dividing in training and validation set. 100 for the training set and 50 for the validation set. \n",
    "validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence[:100]\n",
    "# print(halton_sequence)\n",
    "\n",
    "# print(len(halton_sequence))\n",
    "# print(len(validaton_halton))\n",
    "\n",
    "\n",
    "# Initial A1D\n",
    "import optax\n",
    "\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "learning_rate = 0.0001\n",
    "list_optimizers = [optax.sgd(learning_rate)]\n",
    "# chosing Stochastic Gradient Descent Algorithm.\n",
    "# # We have created a list here keeping in mind that we may apply all the optimizers in optax by storing their objects in the list\n",
    " \n",
    "opt_sgd = list_optimizers[0]\n",
    "opt_state = opt_sgd.init(A1D)\n",
    "\n",
    "params = A1D\n",
    "\n",
    "count = 0\n",
    "data_epoc = 10\n",
    "data_epoc_list = []\n",
    "repetetion = 10\n",
    "# length of halton sequence = 10 \n",
    "\n",
    "tot_eror = 0\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # For Energy Error \n",
    "error_list_4 = [] # For Error \n",
    "\n",
    "flat_halton_sequence = jnp.array(halton_sequence).reshape(-1, 6)\n",
    "\n",
    "# Compute gradient using jacfwd\n",
    "def compute_grads_single(A1D, h_element):\n",
    "    grad_fn = jax.jacfwd(IRK4.find_error)\n",
    "    return grad_fn(A1D, h_element)\n",
    "\n",
    "def compute_error_single(A1D, h_element):\n",
    "    return IRK4.find_error(A1D, h_element)\n",
    "\n",
    "        \n",
    "## Finding the numeric gradient. Jacfwd is not showing good results. \n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        numerical_gradients = numerical_gradients.at[i].set(\n",
    "            (compute_error_single(A1D_plus, halton_element) - compute_error_single(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        )\n",
    "\n",
    "    return numerical_gradients\n",
    "\n",
    "# Use jax.vmap to vectorize the function over the batch\n",
    "compute_grads_batched = jax.vmap(numerical_gradient, in_axes=(None, 0)) # using the numerical gradient instead of jacfwd()\n",
    "compute_error_batched = jax.vmap(compute_error_single, in_axes=(None, 0))\n",
    "\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # Energy Error :\n",
    "error_list_4 = [] # Error :\n",
    "validation_error_list = []\n",
    "tot_error = 0\n",
    "tot_error_energy = 0\n",
    "total_error_e = 0\n",
    "validation_tot_error = 0\n",
    "validation_avg_error = 0\n",
    "\n",
    "with open('S1_output.txt', 'w') as S1_output:\n",
    "    ## Batch Size\n",
    "    batch_size = 100 ## just to remind you right now total halton sequence is also 100, so we are taking the whole set as the batch.\n",
    "    validation_batch_size = 50\n",
    "    \n",
    "    for k in trange(10):\n",
    "        \n",
    "        tot_error = 0\n",
    "        # validation_tot_error = 0\n",
    "        for batch_idx in range(0, len(flat_halton_sequence), batch_size):\n",
    "\n",
    "            # Collect a batch of elements from the flattened Halton sequence\n",
    "            batch_halton = flat_halton_sequence[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # Compute the gradients for the batch using jax.vmap\n",
    "            gradF = compute_grads_batched(A1D, batch_halton)\n",
    "\n",
    "            # Compute the average gradient for the batch\n",
    "            avg_gradF = jnp.mean(gradF, axis=0)\n",
    "\n",
    "            # Perform one step of optimization using the averaged gradient\n",
    "            updates, opt_state = opt_sgd.update(avg_gradF, opt_state)\n",
    "\n",
    "            # Apply the updates to the weights A1D for the entire batch\n",
    "            A1D = optax.apply_updates(A1D, updates)\n",
    "\n",
    "            # Calculate the total error for the batch and accumulate it\n",
    "            batch_error = jnp.mean(compute_error_batched(A1D, batch_halton))\n",
    "            tot_error += batch_error\n",
    "        \"\"\"\n",
    "        # what am i tying to do here ?\n",
    "        \"\"\"\n",
    "        avg_error = tot_error / (len(flat_halton_sequence) // batch_size) # ?\n",
    "        error_list_1.append(avg_error) # ?\n",
    "        \n",
    "        # Write the errors to the respective files\n",
    "        S1_output.write(f\"{avg_error}\\n\")\n",
    "        S1_output.flush()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
