{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noctua 1 : Batch Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from tqdm import trange\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "from jax import jacfwd\n",
    "\n",
    "# Special Transform Functions\n",
    "from jax import grad, jit, vmap, pmap\n",
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "from jax._src.lax.utils import (\n",
    "    _argnum_weak_type,\n",
    "    _input_dtype,\n",
    "    standard_primitive,)\n",
    "from jax._src.lax import lax\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import prk_method.prk_for_optimization as IRK4\n",
    "import Important_functions.Transformation_Functions as TFunctions\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "import Important_functions.Energy_Error as EE\n",
    "\n",
    "A1 = A2 = jnp.array([\n",
    "     [0., 0., 0., 0.],\n",
    "     [5/24, 1/3, -1/24, 0.],\n",
    "     [1/6, 2/3, 1/6, 0.],\n",
    "     [0., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "\n",
    "## Making the Halton code\n",
    "\n",
    "spacedim = [(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5) ]\n",
    "\n",
    "space = Space(spacedim)\n",
    "\n",
    "halton = Halton()\n",
    "n = 1\n",
    "\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "\n",
    "## Dividing in training and validation set. 100 for the training set and 50 for the validation set. \n",
    "# validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence #[:100]\n",
    "print(halton_sequence)\n",
    "\n",
    "# print(len(halton_sequence))\n",
    "# print(len(validaton_halton))\n",
    "\n",
    "\n",
    "\n",
    "# Assuming the necessary functions like IRK4.find_error, convert.Convert_toOneD, and matrix operations are defined\n",
    "\n",
    "\n",
    "# Initial A1D\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Single Halton sequence element\n",
    "halton_element = halton_sequence[0]\n",
    "\n",
    "# Function to compute error\n",
    "def compute_error(A1D, halton_element):\n",
    "    return IRK4.find_error(A1D, halton_element)\n",
    "\n",
    "# Gradient function\n",
    "grad_fn = jax.jacfwd(compute_error)\n",
    "\n",
    "# Central Difference method :\n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        # numerical_gradients[i] = (compute_error(A1D_plus, halton_element) - compute_error(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        \n",
    "        numerical_gradients = numerical_gradients.at[i].set((compute_error(A1D_plus, halton_element) - compute_error(A1D_minus, halton_element)) / (2 * epsilon))\n",
    "\n",
    "    return numerical_gradients\n",
    "\n",
    "# Compute numerical gradients\n",
    "A1D_initial = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "numerical_gradients = numerical_gradient(A1D_initial, halton_element)\n",
    "\n",
    "# Training loop\n",
    "error_list_numerical = []\n",
    "A1D_numerical = A1D_initial.copy()\n",
    "\n",
    "for epoch in trange(100, desc=\"Training with Numerical gradients\"):\n",
    "    # Compute the gradient for the current parameters using the numerical method\n",
    "    gradients = numerical_gradient(A1D_numerical, halton_element)\n",
    "    \n",
    "    # Update the parameters using gradient descent\n",
    "    A1D_numerical = A1D_numerical - learning_rate * gradients\n",
    "    \n",
    "    # Compute the error for the current parameters\n",
    "    current_error = compute_error(A1D_numerical, halton_element)\n",
    "    \n",
    "    # Store the error for plotting\n",
    "    error_list_numerical.append(current_error)\n",
    "\n",
    "# Plot the error\n",
    "import matplotlib.pyplot as plt\n",
    "error_list_numerical = jnp.array(error_list_numerical)\n",
    "plt.plot(range(len(error_list_numerical)), error_list_numerical, label=\"Error\", marker='o')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "# Print final shape of A1D\n",
    "print(A1D.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Learning for PC2 : noctua 1 For different settings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:24<00:50,  1.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m batch_halton \u001b[38;5;241m=\u001b[39m flat_halton_sequence[batch_idx:batch_idx \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Compute the gradients for the batch using jax.vmap\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m gradF \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grads_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA1D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_halton\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Compute the average gradient for the batch\u001b[39;00m\n\u001b[1;32m    157\u001b[0m avg_gradF \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(gradF, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/api.py:1214\u001b[0m, in \u001b[0;36mvmap.<locals>.vmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m in_axes_flat \u001b[38;5;241m=\u001b[39m flatten_axes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap in_axes\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_tree, (in_axes, \u001b[38;5;241m0\u001b[39m), kws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1212\u001b[0m axis_size_ \u001b[38;5;241m=\u001b[39m (axis_size \u001b[38;5;28;01mif\u001b[39;00m axis_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m               _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 1214\u001b[0m out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mbatching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvmap out_axes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspmd_axis_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspmd_axis_name\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree(), out_flat)\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/linear_util.py:192\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen_static_args \u001b[38;5;241m=\u001b[39m out_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m   \u001b[38;5;66;03m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;66;03m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;66;03m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   \u001b[38;5;66;03m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    198\u001b[0m   \u001b[38;5;66;03m# state.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "Cell \u001b[0;32mIn[12], line 119\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[0;34m(A1D, halton_element, epsilon)\u001b[0m\n\u001b[1;32m    116\u001b[0m     A1D_plus \u001b[38;5;241m=\u001b[39m A1D\u001b[38;5;241m.\u001b[39mat[i]\u001b[38;5;241m.\u001b[39mset(A1D[i] \u001b[38;5;241m+\u001b[39m epsilon)\n\u001b[1;32m    117\u001b[0m     A1D_minus \u001b[38;5;241m=\u001b[39m A1D\u001b[38;5;241m.\u001b[39mat[i]\u001b[38;5;241m.\u001b[39mset(A1D[i] \u001b[38;5;241m-\u001b[39m epsilon)\n\u001b[1;32m    118\u001b[0m     numerical_gradients \u001b[38;5;241m=\u001b[39m numerical_gradients\u001b[38;5;241m.\u001b[39mat[i]\u001b[38;5;241m.\u001b[39mset(\n\u001b[0;32m--> 119\u001b[0m         (\u001b[43mcompute_error_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA1D_plus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalton_element\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m compute_error_single(A1D_minus, halton_element)) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m epsilon)\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m numerical_gradients\n",
      "Cell \u001b[0;32mIn[12], line 108\u001b[0m, in \u001b[0;36mcompute_error_single\u001b[0;34m(A1D, h_element)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_error_single\u001b[39m(A1D, h_element):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIRK4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA1D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_element\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/1_Uni_Paderborn/Thesis/Git_Code_Repo/Optimizing_ODE_Solvers_-_A_Machine_Learning_Approach_to_RK4_Enhancement/Rk4_method_implementation/prk_method/prk_for_optimization.py:186\u001b[0m, in \u001b[0;36mfind_error\u001b[0;34m(A1D, H_sequence)\u001b[0m\n\u001b[1;32m    183\u001b[0m time_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# default\u001b[39;00m\n\u001b[1;32m    185\u001b[0m y0 \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mreshape(jnp\u001b[38;5;241m.\u001b[39marray(H_sequence[\u001b[38;5;241m4\u001b[39m]), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# jnp.zeros((1,1)) #\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m z0 \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mreshape(jnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mH_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# jnp.ones((1,1)) #\u001b[39;00m\n\u001b[1;32m    188\u001b[0m istep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    189\u001b[0m NN \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m10\u001b[39m]) \n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py:739\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 739\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py:352\u001b[0m, in \u001b[0;36m_getitem\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m--> 352\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax_numpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rewriting_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:6579\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   6570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rewriting_take\u001b[39m(arr, idx, indices_are_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, unique_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   6571\u001b[0m                     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   6572\u001b[0m   \u001b[38;5;66;03m# Computes arr[idx].\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6576\u001b[0m   \u001b[38;5;66;03m# For simplicity of generated primitives, we call lax.dynamic_slice in the\u001b[39;00m\n\u001b[1;32m   6577\u001b[0m   \u001b[38;5;66;03m# simplest cases: i.e. non-dynamic arrays indexed with integers and slices.\u001b[39;00m\n\u001b[0;32m-> 6579\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (result \u001b[38;5;241m:=\u001b[39m \u001b[43m_attempt_rewriting_take_via_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   6582\u001b[0m   \u001b[38;5;66;03m# TODO(mattjj,dougalm): expand dynamic shape indexing support\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:6566\u001b[0m, in \u001b[0;36m_attempt_rewriting_take_via_slice\u001b[0;34m(arr, idx, mode)\u001b[0m\n\u001b[1;32m   6563\u001b[0m   arr \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mdynamic_slice(\n\u001b[1;32m   6564\u001b[0m       arr, start_indices\u001b[38;5;241m=\u001b[39mstart_indices, slice_sizes\u001b[38;5;241m=\u001b[39mslice_sizes)\n\u001b[1;32m   6565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m int_indices:\n\u001b[0;32m-> 6566\u001b[0m   arr \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mint_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/lax/lax.py:1413\u001b[0m, in \u001b[0;36msqueeze\u001b[0;34m(array, dimensions)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dimensions \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, Array):\n\u001b[1;32m   1412\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m type_cast(Array, array)\n\u001b[0;32m-> 1413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msqueeze_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/core.py:416\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    414\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    415\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 416\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/core.py:419\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 419\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[1;32m    420\u001b[0m     out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n\u001b[1;32m    421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Jax_env/lib/python3.9/site-packages/jax/_src/core.py:1260\u001b[0m, in \u001b[0;36mpop_level\u001b[0;34m(level)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1258\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01myield\u001b[39;00m)\n\u001b[1;32m   1259\u001b[0m prev, thread_local_state\u001b[38;5;241m.\u001b[39mtrace_state\u001b[38;5;241m.\u001b[39mtrace_stack\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m-> 1260\u001b[0m     \u001b[43mthread_local_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_stack\u001b[49m\u001b[38;5;241m.\u001b[39mstack, \\\n\u001b[1;32m   1261\u001b[0m     thread_local_state\u001b[38;5;241m.\u001b[39mtrace_state\u001b[38;5;241m.\u001b[39mtrace_stack\u001b[38;5;241m.\u001b[39mstack[:level]\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)  #double precision\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "from jax import jacfwd\n",
    "\n",
    "# Special Transform Functions\n",
    "from jax import grad, jit, vmap, pmap\n",
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "from jax._src.lax.utils import (\n",
    "    _argnum_weak_type,\n",
    "    _input_dtype,\n",
    "    standard_primitive,)\n",
    "from jax._src.lax import lax\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Test.prk_for_optimization as IRK4\n",
    "import Important_functions.Transformation_Functions as TFunctions\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "import Important_functions.Energy_Error as EE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " : using the initial matrix A and B from the source given below :\n",
    " : wiki link : https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#:~:text=is%5B13%5D-,0,1/6,-A%20slight%20variation\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "############################### Original ##########################################\n",
    "\"\"\"\n",
    "## Lobatto 3A and B fourth order\n",
    "\n",
    "A1 = A2 = jnp.array([\n",
    "     [0., 0., 0., 0.],\n",
    "     [5/24, 1/3, -1/24, 0.],\n",
    "     [1/6, 2/3, 1/6, 0.],\n",
    "     [0., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "\n",
    "\n",
    "## Making the Halton code\n",
    "\n",
    "spacedim = [(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5) ]\n",
    "\n",
    "space = Space(spacedim)\n",
    "\n",
    "halton = Halton()\n",
    "n = 150\n",
    "\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "\n",
    "## Dividing in training and validation set. 100 for the training set and 50 for the validation set. \n",
    "validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence[:100]\n",
    "# print(halton_sequence)\n",
    "\n",
    "# print(len(halton_sequence))\n",
    "# print(len(validaton_halton))\n",
    "\n",
    "\n",
    "# Initial A1D\n",
    "import optax\n",
    "\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "learning_rate = 0.0001\n",
    "list_optimizers = [optax.sgd(learning_rate)]\n",
    "# chosing Stochastic Gradient Descent Algorithm.\n",
    "# # We have created a list here keeping in mind that we may apply all the optimizers in optax by storing their objects in the list\n",
    " \n",
    "opt_sgd = list_optimizers[0]\n",
    "opt_state = opt_sgd.init(A1D)\n",
    "\n",
    "params = A1D\n",
    "\n",
    "count = 0\n",
    "data_epoc = 10\n",
    "data_epoc_list = []\n",
    "repetetion = 10\n",
    "# length of halton sequence = 10 \n",
    "\n",
    "tot_eror = 0\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # For Energy Error \n",
    "error_list_4 = [] # For Error \n",
    "\n",
    "flat_halton_sequence = jnp.array(halton_sequence).reshape(-1, 6)\n",
    "\n",
    "# Compute gradient using jacfwd\n",
    "def compute_grads_single(A1D, h_element):\n",
    "    grad_fn = jax.jacfwd(IRK4.find_error)\n",
    "    return grad_fn(A1D, h_element)\n",
    "\n",
    "def compute_error_single(A1D, h_element):\n",
    "    return IRK4.find_error(A1D, h_element)\n",
    "\n",
    "        \n",
    "## Finding the numeric gradient. Jacfwd is not showing good results. \n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        numerical_gradients = numerical_gradients.at[i].set(\n",
    "            (compute_error_single(A1D_plus, halton_element) - compute_error_single(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        )\n",
    "\n",
    "    return numerical_gradients\n",
    "\n",
    "# Use jax.vmap to vectorize the function over the batch\n",
    "compute_grads_batched = jax.vmap(numerical_gradient, in_axes=(None, 0)) # using the numerical gradient instead of jacfwd()\n",
    "compute_error_batched = jax.vmap(compute_error_single, in_axes=(None, 0))\n",
    "\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # Energy Error :\n",
    "error_list_4 = [] # Error :\n",
    "validation_error_list = []\n",
    "tot_error = 0\n",
    "tot_error_energy = 0\n",
    "total_error_e = 0\n",
    "validation_tot_error = 0\n",
    "validation_avg_error = 0\n",
    "\n",
    "with open('S1_output.txt', 'w') as S1_output:\n",
    "    ## Batch Size\n",
    "    batch_size = 100 ## just to remind you right now total halton sequence is also 100, so we are taking the whole set as the batch.\n",
    "    validation_batch_size = 50\n",
    "    \n",
    "    for k in trange(10):\n",
    "        \n",
    "        tot_error = 0\n",
    "        validation_tot_error = 0\n",
    "        for batch_idx in range(0, len(flat_halton_sequence), batch_size):\n",
    "\n",
    "            # Collect a batch of elements from the flattened Halton sequence\n",
    "            batch_halton = flat_halton_sequence[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # Compute the gradients for the batch using jax.vmap\n",
    "            gradF = compute_grads_batched(A1D, batch_halton)\n",
    "\n",
    "            # Compute the average gradient for the batch\n",
    "            avg_gradF = jnp.mean(gradF, axis=0)\n",
    "\n",
    "            # Perform one step of optimization using the averaged gradient\n",
    "            updates, opt_state = opt_sgd.update(avg_gradF, opt_state)\n",
    "\n",
    "            # Apply the updates to the weights A1D for the entire batch\n",
    "            A1D = optax.apply_updates(A1D, updates)\n",
    "\n",
    "            # Calculate the total error for the batch and accumulate it\n",
    "            batch_error = jnp.mean(compute_error_batched(A1D, batch_halton))\n",
    "            tot_error += batch_error\n",
    "        \"\"\"\n",
    "        # what am i tying to do here ?\n",
    "        \"\"\"\n",
    "        avg_error = tot_error / (len(flat_halton_sequence) // batch_size) # ?\n",
    "        error_list_1.append(avg_error) # ?\n",
    "        \n",
    "        # Write the errors to the respective files\n",
    "        S1_output.write(f\"{avg_error}\\n\")\n",
    "        S1_output.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Noctua 1, making fori loop in the optimization file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)  #double precision\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "from jax import jacfwd\n",
    "\n",
    "# Special Transform Functions\n",
    "from jax import grad, jit, vmap, pmap\n",
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "from jax._src.lax.utils import (\n",
    "    _argnum_weak_type,\n",
    "    _input_dtype,\n",
    "    standard_primitive,)\n",
    "from jax._src.lax import lax\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Test_prk_for_optimization as IRK4\n",
    "import Important_functions.Transformation_Functions as TFunctions\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "import Important_functions.Energy_Error as EE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " : using the initial matrix A and B from the source given below :\n",
    " : wiki link : https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#:~:text=is%5B13%5D-,0,1/6,-A%20slight%20variation\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "############################### Original ##########################################\n",
    "\"\"\"\n",
    "## Lobatto 3A and B fourth order\n",
    "\n",
    "A1 = A2 = jnp.array([\n",
    "     [0., 0., 0., 0.],\n",
    "     [5/24, 1/3, -1/24, 0.],\n",
    "     [1/6, 2/3, 1/6, 0.],\n",
    "     [0., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "\n",
    "\n",
    "## Making the Halton code\n",
    "\n",
    "spacedim = [(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5) ]\n",
    "\n",
    "space = Space(spacedim)\n",
    "\n",
    "halton = Halton()\n",
    "n = 150\n",
    "\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "\n",
    "## Dividing in training and validation set. 100 for the training set and 50 for the validation set. \n",
    "validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence[:100]\n",
    "# print(halton_sequence)\n",
    "\n",
    "# print(len(halton_sequence))\n",
    "# print(len(validaton_halton))\n",
    "\n",
    "\n",
    "# Initial A1D\n",
    "import optax\n",
    "\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "learning_rate = 0.0001\n",
    "list_optimizers = [optax.sgd(learning_rate)]\n",
    "# chosing Stochastic Gradient Descent Algorithm.\n",
    "# # We have created a list here keeping in mind that we may apply all the optimizers in optax by storing their objects in the list\n",
    " \n",
    "opt_sgd = list_optimizers[0]\n",
    "opt_state = opt_sgd.init(A1D)\n",
    "\n",
    "params = A1D\n",
    "\n",
    "count = 0\n",
    "data_epoc = 10\n",
    "data_epoc_list = []\n",
    "repetetion = 10\n",
    "# length of halton sequence = 10 \n",
    "\n",
    "tot_eror = 0\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # For Energy Error \n",
    "error_list_4 = [] # For Error \n",
    "\n",
    "flat_halton_sequence = jnp.array(halton_sequence).reshape(-1, 6)\n",
    "\n",
    "# Compute gradient using jacfwd\n",
    "def compute_grads_single(A1D, h_element):\n",
    "    grad_fn = jax.jacfwd(IRK4.find_error)\n",
    "    return grad_fn(A1D, h_element)\n",
    "\n",
    "def compute_error_single(A1D, h_element):\n",
    "    return IRK4.find_error(A1D, h_element)\n",
    "\n",
    "        \n",
    "## Finding the numeric gradient. Jacfwd is not showing good results. \n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        numerical_gradients = numerical_gradients.at[i].set(\n",
    "            (compute_error_single(A1D_plus, halton_element) - compute_error_single(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        )\n",
    "\n",
    "    return numerical_gradients\n",
    "\n",
    "# Use jax.vmap to vectorize the function over the batch\n",
    "compute_grads_batched = jax.vmap(numerical_gradient, in_axes=(None, 0)) # using the numerical gradient instead of jacfwd()\n",
    "compute_error_batched = jax.vmap(compute_error_single, in_axes=(None, 0))\n",
    "\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # Energy Error :\n",
    "error_list_4 = [] # Error :\n",
    "validation_error_list = []\n",
    "tot_error = 0\n",
    "tot_error_energy = 0\n",
    "total_error_e = 0\n",
    "validation_tot_error = 0\n",
    "validation_avg_error = 0\n",
    "\n",
    "with open('S1_output.txt', 'w') as S1_output:\n",
    "    ## Batch Size\n",
    "    batch_size = 100 ## just to remind you right now total halton sequence is also 100, so we are taking the whole set as the batch.\n",
    "    validation_batch_size = 50\n",
    "    \n",
    "    for k in trange(10):\n",
    "        \n",
    "        tot_error = 0\n",
    "        validation_tot_error = 0\n",
    "        for batch_idx in range(0, len(flat_halton_sequence), batch_size):\n",
    "\n",
    "            # Collect a batch of elements from the flattened Halton sequence\n",
    "            batch_halton = flat_halton_sequence[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # Compute the gradients for the batch using jax.vmap\n",
    "            gradF = compute_grads_batched(A1D, batch_halton)\n",
    "\n",
    "            # Compute the average gradient for the batch\n",
    "            avg_gradF = jnp.mean(gradF, axis=0)\n",
    "\n",
    "            # Perform one step of optimization using the averaged gradient\n",
    "            updates, opt_state = opt_sgd.update(avg_gradF, opt_state)\n",
    "\n",
    "            # Apply the updates to the weights A1D for the entire batch\n",
    "            A1D = optax.apply_updates(A1D, updates)\n",
    "\n",
    "            # Calculate the total error for the batch and accumulate it\n",
    "            batch_error = jnp.mean(compute_error_batched(A1D, batch_halton))\n",
    "            tot_error += batch_error\n",
    "        \"\"\"\n",
    "        # what am i tying to do here ?\n",
    "        \"\"\"\n",
    "        avg_error = tot_error / (len(flat_halton_sequence) // batch_size) # ?\n",
    "        error_list_1.append(avg_error) # ?\n",
    "        \n",
    "        # Write the errors to the respective files\n",
    "        S1_output.write(f\"{avg_error}\\n\")\n",
    "        S1_output.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "[ 0.          0.          0.          0.          0.20833333  0.33333333\n",
      " -0.04166667  0.          0.16666667  0.66666667  0.16666667  0.\n",
      "  0.          0.          0.          0.          0.16666667  0.66666667\n",
      "  0.16666667  0.          0.          0.          0.          0.\n",
      "  0.20833333  0.33333333 -0.04166667  0.          0.16666667  0.66666667\n",
      "  0.16666667  0.          0.          0.          0.          0.\n",
      "  0.16666667  0.66666667  0.16666667  0.        ]\n",
      "[ 0.          0.          0.          0.          0.20833333  0.33333333\n",
      " -0.04166667  0.          0.16666667  0.66666667  0.16666667  0.\n",
      "  0.          0.          0.          0.          0.16666667  0.66666667\n",
      "  0.16666667  0.          0.          0.          0.          0.\n",
      "  0.20833333  0.33333333 -0.04166667  0.          0.16666667  0.66666667\n",
      "  0.16666667  0.          0.          0.          0.          0.\n",
      "  0.16666667  0.66666667  0.16666667  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import config, lax\n",
    "config.update(\"jax_enable_x64\", True)  # double precision\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "import optax\n",
    "\n",
    "import Test_prk_for_optimization as IRK4\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "\n",
    "# Initial A1D weights\n",
    "A1 = A2 = jnp.array([\n",
    "    [0., 0., 0., 0.],\n",
    "    [5/24, 1/3, -1/24, 0.],\n",
    "    [1/6, 2/3, 1/6, 0.],\n",
    "    [0., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "# Generate Halton sequence\n",
    "spacedim = [(-1.0, 0.5), (-1.0, 0.5), (-1.0, 0.5), (-1.0, 0.5), (-1.0, 0.5), (-1.0, 0.5)]\n",
    "space = Space(spacedim)\n",
    "halton = Halton()\n",
    "n = 150\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence[:100]\n",
    "\n",
    "# Initial A1D\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "opt_sgd = optax.sgd(learning_rate)\n",
    "opt_state = opt_sgd.init(A1D)\n",
    "\n",
    "flat_halton_sequence = jnp.array(halton_sequence).reshape(-1, 6)\n",
    "\n",
    "# Define the gradient computation function\n",
    "def compute_grads_single(A1D, h_element):\n",
    "    grad_fn = jax.jacfwd(IRK4.find_error)\n",
    "    return grad_fn(A1D, h_element)\n",
    "\n",
    "def compute_error_single(A1D, h_element):\n",
    "    return IRK4.find_error(A1D, h_element)\n",
    "\n",
    "# Finding the numeric gradient\n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        numerical_gradients = numerical_gradients.at[i].set(\n",
    "            (compute_error_single(A1D_plus, halton_element) - compute_error_single(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        )\n",
    "    return numerical_gradients\n",
    "\n",
    "# Vectorize the function over the batch\n",
    "compute_grads_batched = jax.vmap(numerical_gradient, in_axes=(None, 0))\n",
    "compute_error_batched = jax.vmap(compute_error_single, in_axes=(None, 0))\n",
    "\n",
    "# Precompute the batch indices\n",
    "num_batches = len(flat_halton_sequence) // 100\n",
    "\n",
    "# Define the inner loop body function\n",
    "def inner_loop_body(batch_idx, state):\n",
    "    A1D, opt_state, tot_error = state\n",
    "    # Collect a batch of elements from the flattened Halton sequence using dynamic slice\n",
    "    start_idx = batch_idx * 100\n",
    "    end_idx = start_idx + 100\n",
    "    batch_halton = flat_halton_sequence[start_idx:end_idx]\n",
    "    # Compute the gradients for the batch using jax.vmap\n",
    "    gradF = compute_grads_batched(A1D, batch_halton)\n",
    "    # Compute the average gradient for the batch\n",
    "    avg_gradF = jnp.mean(gradF, axis=0)\n",
    "    # Perform one step of optimization using the averaged gradient\n",
    "    updates, opt_state = opt_sgd.update(avg_gradF, opt_state)\n",
    "    # Apply the updates to the weights A1D for the entire batch\n",
    "    A1D = optax.apply_updates(A1D, updates)\n",
    "    # Calculate the total error for the batch and accumulate it\n",
    "    batch_error = jnp.mean(compute_error_batched(A1D, batch_halton))\n",
    "    tot_error += batch_error\n",
    "    return A1D, opt_state, tot_error\n",
    "\n",
    "# Define the outer loop body function using lax.scan\n",
    "def outer_loop_body(i, state):\n",
    "    A1D, opt_state, tot_error, b_size, l_Halton, flat_halton_sequence, batch_halton = state\n",
    "    \n",
    "    max_iter_inner_loop = l_Halton // b_size\n",
    "    \n",
    "    state_inner_loop = (A1D, opt_state, tot_error, batch_halton)\n",
    "    # Run the inner loop using lax.scan\n",
    "    # state, _ = jax.lax.fori_loop(0, max_iter_inner_loop, inner_loop_body, state)\n",
    "\n",
    "    # # Extract updated values from the state\n",
    "    # A1D, opt_state, tot_error = state\n",
    "    # Save the latest A1D values to the weights file\n",
    "    np_array_A1D = jnp.array(tot_error)\n",
    "    np_array_A1D_string = str(np_array_A1D) # ' '.join(map(str, np_array_A1D))\n",
    "    with open('S1_Final_weights(Perturbed_high).txt', 'w') as S1_weights:\n",
    "        S1_weights.write(np_array_A1D_string)\n",
    "    \n",
    "\n",
    "    # # Calculate and save the average error\n",
    "    # avg_error = tot_error / num_batches\n",
    "    # with open('S1_Error(Perturbed_high).txt', 'a') as S1_output:\n",
    "    #     S1_output.write(f\"{avg_error}\\n\")\n",
    "\n",
    "    return state  # Reset tot_error for the next outer loop iteration\n",
    "\n",
    "# Initialize the loop variables, \n",
    "# also adding the batch size and the length of the halton sequence\n",
    "len_halton = len(flat_halton_sequence)\n",
    "batch_size = 100\n",
    "batch_halton = flat_halton_sequence[0 : batch_size]\n",
    "\n",
    "print(A1D)\n",
    "state = (A1D, opt_state, 0.0, batch_size, len_halton, flat_halton_sequence, batch_halton)\n",
    "\n",
    "# Perform the optimization loop using lax.fori_loop\n",
    "state = jax.lax.fori_loop(0, 10, outer_loop_body, state)\n",
    "A1D, opt_state, _,_,_,_,_ = state\n",
    "\n",
    "print(A1D)\n",
    "\n",
    "\"\"\" \n",
    "Ravi read this : Storing of each and every error wont be possible in jax loop\n",
    "i will have to create a list and store all the answers there \n",
    "it will only be accessiable after the loop is completed, which wont be a good idea ,\n",
    "\n",
    "as right now i am also check the output as the program runs \n",
    "\n",
    "so for now let the optimization use the normal loop and later towards the end i can\n",
    "change it to jax fori loop.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Save the latest A1D values to the weights file\n",
    "# np_array_A1D = jnp.array(A1D)\n",
    "# np_array_A1D_string = ' '.join(map(str, np_array_A1D))\n",
    "# with open('S1_Final_weights(Perturbed_high).txt', 'w') as S1_weights:\n",
    "#     S1_weights.write(np_array_A1D_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "# Simple jax fori loop program :\n",
    "\n",
    "def test_Fori(i, state):\n",
    "    s = state\n",
    "    s = s + s\n",
    "    state = s\n",
    "    return state\n",
    "\n",
    "sum1 = 1\n",
    "state = (sum1)\n",
    "final_state = jax.lax.fori_loop(0,11, test_Fori, state)\n",
    "print(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "Traced<ShapedArray(int64[5])>with<DynamicJaxprTrace(level=1/0)> /n\n",
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Initialize two JAX arrays\n",
    "array1 = jnp.array([1, 2, 3, 4, 5])\n",
    "array2 = jnp.zeros_like(array1)\n",
    "\n",
    "print(array2)\n",
    "\n",
    "def print_func(n):\n",
    "    print(n, \"/n\")\n",
    "# Define the loop body function\n",
    "def loop_body(i, array2):\n",
    "    print_func(array2) # this wont work\n",
    "    return array2.at[i].set(array1[i])\n",
    "\n",
    "# Perform the loop using lax.fori_loop\n",
    "array2 = jax.lax.fori_loop(0, len(array1), loop_body, array2)\n",
    "\n",
    "print(array2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
