{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noctua 1 : Batch Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from tqdm import trange\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "from jax import jacfwd\n",
    "\n",
    "# Special Transform Functions\n",
    "from jax import grad, jit, vmap, pmap\n",
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "from jax._src.lax.utils import (\n",
    "    _argnum_weak_type,\n",
    "    _input_dtype,\n",
    "    standard_primitive,)\n",
    "from jax._src.lax import lax\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import prk_method.prk_for_optimization as IRK4\n",
    "import Important_functions.Transformation_Functions as TFunctions\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "import Important_functions.Energy_Error as EE\n",
    "\n",
    "A1 = A2 = jnp.array([\n",
    "     [0., 0., 0., 0.],\n",
    "     [5/24, 1/3, -1/24, 0.],\n",
    "     [1/6, 2/3, 1/6, 0.],\n",
    "     [0., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "\n",
    "## Making the Halton code\n",
    "\n",
    "spacedim = [(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5) ]\n",
    "\n",
    "space = Space(spacedim)\n",
    "\n",
    "halton = Halton()\n",
    "n = 1\n",
    "\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "\n",
    "## Dividing in training and validation set. 100 for the training set and 50 for the validation set. \n",
    "# validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence #[:100]\n",
    "print(halton_sequence)\n",
    "\n",
    "# print(len(halton_sequence))\n",
    "# print(len(validaton_halton))\n",
    "\n",
    "\n",
    "\n",
    "# Assuming the necessary functions like IRK4.find_error, convert.Convert_toOneD, and matrix operations are defined\n",
    "\n",
    "\n",
    "# Initial A1D\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Single Halton sequence element\n",
    "halton_element = halton_sequence[0]\n",
    "\n",
    "# Function to compute error\n",
    "def compute_error(A1D, halton_element):\n",
    "    return IRK4.find_error(A1D, halton_element)\n",
    "\n",
    "# Gradient function\n",
    "grad_fn = jax.jacfwd(compute_error)\n",
    "\n",
    "# Central Difference method :\n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        # numerical_gradients[i] = (compute_error(A1D_plus, halton_element) - compute_error(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        \n",
    "        numerical_gradients = numerical_gradients.at[i].set((compute_error(A1D_plus, halton_element) - compute_error(A1D_minus, halton_element)) / (2 * epsilon))\n",
    "\n",
    "    return numerical_gradients\n",
    "\n",
    "# Compute numerical gradients\n",
    "A1D_initial = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "numerical_gradients = numerical_gradient(A1D_initial, halton_element)\n",
    "\n",
    "# Training loop\n",
    "error_list_numerical = []\n",
    "A1D_numerical = A1D_initial.copy()\n",
    "\n",
    "for epoch in trange(100, desc=\"Training with Numerical gradients\"):\n",
    "    # Compute the gradient for the current parameters using the numerical method\n",
    "    gradients = numerical_gradient(A1D_numerical, halton_element)\n",
    "    \n",
    "    # Update the parameters using gradient descent\n",
    "    A1D_numerical = A1D_numerical - learning_rate * gradients\n",
    "    \n",
    "    # Compute the error for the current parameters\n",
    "    current_error = compute_error(A1D_numerical, halton_element)\n",
    "    \n",
    "    # Store the error for plotting\n",
    "    error_list_numerical.append(current_error)\n",
    "\n",
    "# Plot the error\n",
    "import matplotlib.pyplot as plt\n",
    "error_list_numerical = jnp.array(error_list_numerical)\n",
    "plt.plot(range(len(error_list_numerical)), error_list_numerical, label=\"Error\", marker='o')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "# Print final shape of A1D\n",
    "print(A1D.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Learning for PC2 : noctua 1 For different settings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)  #double precision\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "from jax import jacfwd\n",
    "\n",
    "# Special Transform Functions\n",
    "from jax import grad, jit, vmap, pmap\n",
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "from jax._src.lax.utils import (\n",
    "    _argnum_weak_type,\n",
    "    _input_dtype,\n",
    "    standard_primitive,)\n",
    "from jax._src.lax import lax\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Test_prk_for_optimization as IRK4\n",
    "import Important_functions.Transformation_Functions as TFunctions\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "import Important_functions.Energy_Error as EE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " : using the initial matrix A and B from the source given below :\n",
    " : wiki link : https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#:~:text=is%5B13%5D-,0,1/6,-A%20slight%20variation\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "############################### Original ##########################################\n",
    "Important detail noticed :\n",
    "\n",
    "The energy error is of the range 10^-6\n",
    "\"\"\"\n",
    "## Lobatto 3A and B fourth order\n",
    "\n",
    "# A1 = A2 = jnp.array([\n",
    "#      [0., 0., 0., 0.],\n",
    "#      [5/24, 1/3, -1/24, 0.],\n",
    "#      [1/6, 2/3, 1/6, 0.],\n",
    "#      [0., 0., 0., 0.]])\n",
    "# B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Perturbed \n",
    "Important detailed noticed :\n",
    "The energy error is of order : 10^-4\n",
    "\n",
    "\"\"\"\n",
    "A1 = A2 = jnp.array([\n",
    "     [0., 1., 1/3, 1/2],\n",
    "     [7/24, 2/3, -1/24, 0.],\n",
    "     [3/6, 1/3, 1/6, 0.],\n",
    "     [1., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([2/6, 1/3, 4/6, 1.])\n",
    "\n",
    "## Making the Halton code\n",
    "\n",
    "spacedim = [(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5) ]\n",
    "\n",
    "space = Space(spacedim)\n",
    "\n",
    "halton = Halton()\n",
    "n = 150\n",
    "\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "\n",
    "## Dividing in training and validation set. 100 for the training set and 50 for the validation set. \n",
    "validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence[:100]\n",
    "# print(halton_sequence)\n",
    "\n",
    "# print(len(halton_sequence))\n",
    "# print(len(validaton_halton))\n",
    "\n",
    "\n",
    "# Initial A1D\n",
    "import optax\n",
    "\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "learning_rate = 0.0001\n",
    "list_optimizers = [optax.sgd(learning_rate)]\n",
    "# chosing Stochastic Gradient Descent Algorithm.\n",
    "# # We have created a list here keeping in mind that we may apply all the optimizers in optax by storing their objects in the list\n",
    " \n",
    "opt_sgd = list_optimizers[0]\n",
    "opt_state = opt_sgd.init(A1D)\n",
    "\n",
    "params = A1D\n",
    "\n",
    "count = 0\n",
    "data_epoc = 10\n",
    "data_epoc_list = []\n",
    "repetetion = 10\n",
    "# length of halton sequence = 10 \n",
    "\n",
    "tot_eror = 0\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # For Energy Error \n",
    "error_list_4 = [] # For Error \n",
    "\n",
    "flat_halton_sequence = jnp.array(halton_sequence).reshape(-1, 6)\n",
    "\n",
    "# Compute gradient using jacfwd\n",
    "def compute_grads_single(A1D, h_element):\n",
    "    grad_fn = jax.jacfwd(IRK4.find_error)\n",
    "    return grad_fn(A1D, h_element)\n",
    "\n",
    "def compute_error_single(A1D, h_element):\n",
    "    # print(IRK4.find_error(A1D, h_element)[0])\n",
    "    return IRK4.find_error(A1D, h_element)#[0]\n",
    "\n",
    "        \n",
    "## Finding the numeric gradient. Jacfwd is not showing good results. \n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        numerical_gradients = numerical_gradients.at[i].set(\n",
    "            (compute_error_single(A1D_plus, halton_element) - compute_error_single(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        )\n",
    "\n",
    "    return numerical_gradients\n",
    "\n",
    "# Use jax.vmap to vectorize the function over the batch\n",
    "compute_grads_batched = jax.vmap(numerical_gradient, in_axes=(None, 0)) # using the numerical gradient instead of jacfwd()\n",
    "compute_error_batched = jax.vmap(compute_error_single, in_axes=(None, 0))\n",
    "\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # Energy Error :\n",
    "error_list_4 = [] # Error :\n",
    "validation_error_list = []\n",
    "tot_error = 0\n",
    "tot_error_energy = 0\n",
    "total_error_e = 0\n",
    "validation_tot_error = 0\n",
    "validation_avg_error = 0\n",
    "\n",
    "with open('S1_output.txt', 'w') as S1_output:\n",
    "    ## Batch Size\n",
    "    batch_size = 100 ## just to remind you right now total halton sequence is also 100, so we are taking the whole set as the batch.\n",
    "    validation_batch_size = 50\n",
    "    \n",
    "    for k in trange(10):\n",
    "        \n",
    "        tot_error = 0\n",
    "        validation_tot_error = 0\n",
    "        for batch_idx in range(0, len(flat_halton_sequence), batch_size):\n",
    "\n",
    "            # Collect a batch of elements from the flattened Halton sequence\n",
    "            batch_halton = flat_halton_sequence[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # Compute the gradients for the batch using jax.vmap\n",
    "            gradF = compute_grads_batched(A1D, batch_halton)\n",
    "\n",
    "            # Compute the average gradient for the batch\n",
    "            avg_gradF = jnp.mean(gradF, axis=0)\n",
    "\n",
    "            # Perform one step of optimization using the averaged gradient\n",
    "            updates, opt_state = opt_sgd.update(avg_gradF, opt_state)\n",
    "\n",
    "            # Apply the updates to the weights A1D for the entire batch\n",
    "            A1D = optax.apply_updates(A1D, updates)\n",
    "\n",
    "            # Calculate the total error for the batch and accumulate it\n",
    "            batch_error = jnp.mean(compute_error_batched(A1D, batch_halton))\n",
    "            tot_error += batch_error\n",
    "        \"\"\"\n",
    "        # what am i tying to do here ?\n",
    "        \"\"\"\n",
    "        avg_error = tot_error / (len(flat_halton_sequence) // batch_size) # ?\n",
    "        error_list_1.append(avg_error) # ?\n",
    "        \n",
    "        # Write the errors to the respective files\n",
    "        S1_output.write(f\"{avg_error}\\n\")\n",
    "        S1_output.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Noctua 1, making fori loop in the optimization file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)  #double precision\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "from jax import jacfwd\n",
    "\n",
    "# Special Transform Functions\n",
    "from jax import grad, jit, vmap, pmap\n",
    "import jax\n",
    "from jax import jit\n",
    "\n",
    "from jax._src.lax.utils import (\n",
    "    _argnum_weak_type,\n",
    "    _input_dtype,\n",
    "    standard_primitive,)\n",
    "from jax._src.lax import lax\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Test_prk_for_optimization as IRK4\n",
    "import Important_functions.Transformation_Functions as TFunctions\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "import Important_functions.Energy_Error as EE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " : using the initial matrix A and B from the source given below :\n",
    " : wiki link : https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#:~:text=is%5B13%5D-,0,1/6,-A%20slight%20variation\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "############################### Original ##########################################\n",
    "\"\"\"\n",
    "## Lobatto 3A and B fourth order\n",
    "\n",
    "A1 = A2 = jnp.array([\n",
    "     [0., 0., 0., 0.],\n",
    "     [5/24, 1/3, -1/24, 0.],\n",
    "     [1/6, 2/3, 1/6, 0.],\n",
    "     [0., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "\n",
    "\n",
    "## Making the Halton code\n",
    "\n",
    "spacedim = [(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5),(-1.0, 0.5) ]\n",
    "\n",
    "space = Space(spacedim)\n",
    "\n",
    "halton = Halton()\n",
    "n = 150\n",
    "\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "\n",
    "## Dividing in training and validation set. 100 for the training set and 50 for the validation set. \n",
    "validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence[:100]\n",
    "# print(halton_sequence)\n",
    "\n",
    "# print(len(halton_sequence))\n",
    "# print(len(validaton_halton))\n",
    "\n",
    "\n",
    "# Initial A1D\n",
    "import optax\n",
    "\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "learning_rate = 0.0001\n",
    "list_optimizers = [optax.sgd(learning_rate)]\n",
    "# chosing Stochastic Gradient Descent Algorithm.\n",
    "# # We have created a list here keeping in mind that we may apply all the optimizers in optax by storing their objects in the list\n",
    " \n",
    "opt_sgd = list_optimizers[0]\n",
    "opt_state = opt_sgd.init(A1D)\n",
    "\n",
    "params = A1D\n",
    "\n",
    "count = 0\n",
    "data_epoc = 10\n",
    "data_epoc_list = []\n",
    "repetetion = 10\n",
    "# length of halton sequence = 10 \n",
    "\n",
    "tot_eror = 0\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # For Energy Error \n",
    "error_list_4 = [] # For Error \n",
    "\n",
    "flat_halton_sequence = jnp.array(halton_sequence).reshape(-1, 6)\n",
    "\n",
    "# Compute gradient using jacfwd\n",
    "def compute_grads_single(A1D, h_element):\n",
    "    grad_fn = jax.jacfwd(IRK4.find_error)\n",
    "    return grad_fn(A1D, h_element)\n",
    "\n",
    "def compute_error_single(A1D, h_element):\n",
    "    return IRK4.find_error(A1D, h_element)\n",
    "\n",
    "        \n",
    "## Finding the numeric gradient. Jacfwd is not showing good results. \n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        numerical_gradients = numerical_gradients.at[i].set(\n",
    "            (compute_error_single(A1D_plus, halton_element) - compute_error_single(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        )\n",
    "\n",
    "    return numerical_gradients\n",
    "\n",
    "# Use jax.vmap to vectorize the function over the batch\n",
    "compute_grads_batched = jax.vmap(numerical_gradient, in_axes=(None, 0)) # using the numerical gradient instead of jacfwd()\n",
    "compute_error_batched = jax.vmap(compute_error_single, in_axes=(None, 0))\n",
    "\n",
    "error_list_1 = [] \n",
    "error_list_2 = []\n",
    "error_list_3 = [] # Energy Error :\n",
    "error_list_4 = [] # Error :\n",
    "validation_error_list = []\n",
    "tot_error = 0\n",
    "tot_error_energy = 0\n",
    "total_error_e = 0\n",
    "validation_tot_error = 0\n",
    "validation_avg_error = 0\n",
    "\n",
    "with open('S1_output.txt', 'w') as S1_output:\n",
    "    ## Batch Size\n",
    "    batch_size = 100 ## just to remind you right now total halton sequence is also 100, so we are taking the whole set as the batch.\n",
    "    validation_batch_size = 50\n",
    "    \n",
    "    for k in trange(10):\n",
    "        \n",
    "        tot_error = 0\n",
    "        validation_tot_error = 0\n",
    "        for batch_idx in range(0, len(flat_halton_sequence), batch_size):\n",
    "\n",
    "            # Collect a batch of elements from the flattened Halton sequence\n",
    "            batch_halton = flat_halton_sequence[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            # Compute the gradients for the batch using jax.vmap\n",
    "            gradF = compute_grads_batched(A1D, batch_halton)\n",
    "\n",
    "            # Compute the average gradient for the batch\n",
    "            avg_gradF = jnp.mean(gradF, axis=0)\n",
    "\n",
    "            # Perform one step of optimization using the averaged gradient\n",
    "            updates, opt_state = opt_sgd.update(avg_gradF, opt_state)\n",
    "\n",
    "            # Apply the updates to the weights A1D for the entire batch\n",
    "            A1D = optax.apply_updates(A1D, updates)\n",
    "\n",
    "            # Calculate the total error for the batch and accumulate it\n",
    "            batch_error = jnp.mean(compute_error_batched(A1D, batch_halton))\n",
    "            tot_error += batch_error\n",
    "        \"\"\"\n",
    "        # what am i tying to do here ?\n",
    "        \"\"\"\n",
    "        avg_error = tot_error / (len(flat_halton_sequence) // batch_size) # ?\n",
    "        error_list_1.append(avg_error) # ?\n",
    "        \n",
    "        # Write the errors to the respective files\n",
    "        S1_output.write(f\"{avg_error}\\n\")\n",
    "        S1_output.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "[ 0.          0.          0.          0.          0.20833333  0.33333333\n",
      " -0.04166667  0.          0.16666667  0.66666667  0.16666667  0.\n",
      "  0.          0.          0.          0.          0.16666667  0.66666667\n",
      "  0.16666667  0.          0.          0.          0.          0.\n",
      "  0.20833333  0.33333333 -0.04166667  0.          0.16666667  0.66666667\n",
      "  0.16666667  0.          0.          0.          0.          0.\n",
      "  0.16666667  0.66666667  0.16666667  0.        ]\n",
      "[ 0.          0.          0.          0.          0.20833333  0.33333333\n",
      " -0.04166667  0.          0.16666667  0.66666667  0.16666667  0.\n",
      "  0.          0.          0.          0.          0.16666667  0.66666667\n",
      "  0.16666667  0.          0.          0.          0.          0.\n",
      "  0.20833333  0.33333333 -0.04166667  0.          0.16666667  0.66666667\n",
      "  0.16666667  0.          0.          0.          0.          0.\n",
      "  0.16666667  0.66666667  0.16666667  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import config, lax\n",
    "config.update(\"jax_enable_x64\", True)  # double precision\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from skopt.space import Space\n",
    "from skopt.sampler import Halton\n",
    "import optax\n",
    "\n",
    "import Test_prk_for_optimization as IRK4\n",
    "import Important_functions.Convert_1D2D as convert\n",
    "\n",
    "# Initial A1D weights\n",
    "A1 = A2 = jnp.array([\n",
    "    [0., 0., 0., 0.],\n",
    "    [5/24, 1/3, -1/24, 0.],\n",
    "    [1/6, 2/3, 1/6, 0.],\n",
    "    [0., 0., 0., 0.]])\n",
    "B1 = B2 = jnp.array([1/6, 2/3, 1/6, 0.])\n",
    "\n",
    "# Generate Halton sequence\n",
    "spacedim = [(-1.0, 0.5), (-1.0, 0.5), (-1.0, 0.5), (-1.0, 0.5), (-1.0, 0.5), (-1.0, 0.5)]\n",
    "space = Space(spacedim)\n",
    "halton = Halton()\n",
    "n = 150\n",
    "halton_sequence = halton.generate(space, n)\n",
    "halton_sequence = jnp.array(halton_sequence)\n",
    "validation_halton = halton_sequence[100:150]\n",
    "halton_sequence = halton_sequence[:100]\n",
    "\n",
    "# Initial A1D\n",
    "A1D = convert.Convert_toOneD(A1, A2, B1, B2)\n",
    "print(A1D.shape)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "opt_sgd = optax.sgd(learning_rate)\n",
    "opt_state = opt_sgd.init(A1D)\n",
    "\n",
    "flat_halton_sequence = jnp.array(halton_sequence).reshape(-1, 6)\n",
    "\n",
    "# Define the gradient computation function\n",
    "def compute_grads_single(A1D, h_element):\n",
    "    grad_fn = jax.jacfwd(IRK4.find_error)\n",
    "    return grad_fn(A1D, h_element)\n",
    "\n",
    "def compute_error_single(A1D, h_element):\n",
    "    return IRK4.find_error(A1D, h_element)\n",
    "\n",
    "# Finding the numeric gradient\n",
    "def numerical_gradient(A1D, halton_element, epsilon=1e-5):\n",
    "    numerical_gradients = jnp.zeros_like(A1D)\n",
    "    for i in range(len(A1D)):\n",
    "        A1D_plus = A1D.at[i].set(A1D[i] + epsilon)\n",
    "        A1D_minus = A1D.at[i].set(A1D[i] - epsilon)\n",
    "        numerical_gradients = numerical_gradients.at[i].set(\n",
    "            (compute_error_single(A1D_plus, halton_element) - compute_error_single(A1D_minus, halton_element)) / (2 * epsilon)\n",
    "        )\n",
    "    return numerical_gradients\n",
    "\n",
    "# Vectorize the function over the batch\n",
    "compute_grads_batched = jax.vmap(numerical_gradient, in_axes=(None, 0))\n",
    "compute_error_batched = jax.vmap(compute_error_single, in_axes=(None, 0))\n",
    "\n",
    "# Precompute the batch indices\n",
    "num_batches = len(flat_halton_sequence) // 100\n",
    "\n",
    "# Define the inner loop body function\n",
    "def inner_loop_body(batch_idx, state):\n",
    "    A1D, opt_state, tot_error = state\n",
    "    # Collect a batch of elements from the flattened Halton sequence using dynamic slice\n",
    "    start_idx = batch_idx * 100\n",
    "    end_idx = start_idx + 100\n",
    "    batch_halton = flat_halton_sequence[start_idx:end_idx]\n",
    "    # Compute the gradients for the batch using jax.vmap\n",
    "    gradF = compute_grads_batched(A1D, batch_halton)\n",
    "    # Compute the average gradient for the batch\n",
    "    avg_gradF = jnp.mean(gradF, axis=0)\n",
    "    # Perform one step of optimization using the averaged gradient\n",
    "    updates, opt_state = opt_sgd.update(avg_gradF, opt_state)\n",
    "    # Apply the updates to the weights A1D for the entire batch\n",
    "    A1D = optax.apply_updates(A1D, updates)\n",
    "    # Calculate the total error for the batch and accumulate it\n",
    "    batch_error = jnp.mean(compute_error_batched(A1D, batch_halton))\n",
    "    tot_error += batch_error\n",
    "    return A1D, opt_state, tot_error\n",
    "\n",
    "# Define the outer loop body function using lax.scan\n",
    "def outer_loop_body(i, state):\n",
    "    A1D, opt_state, tot_error, b_size, l_Halton, flat_halton_sequence, batch_halton = state\n",
    "    \n",
    "    max_iter_inner_loop = l_Halton // b_size\n",
    "    \n",
    "    state_inner_loop = (A1D, opt_state, tot_error, batch_halton)\n",
    "    # Run the inner loop using lax.scan\n",
    "    # state, _ = jax.lax.fori_loop(0, max_iter_inner_loop, inner_loop_body, state)\n",
    "\n",
    "    # # Extract updated values from the state\n",
    "    # A1D, opt_state, tot_error = state\n",
    "    # Save the latest A1D values to the weights file\n",
    "    np_array_A1D = jnp.array(tot_error)\n",
    "    np_array_A1D_string = str(np_array_A1D) # ' '.join(map(str, np_array_A1D))\n",
    "    with open('S1_Final_weights(Perturbed_high).txt', 'w') as S1_weights:\n",
    "        S1_weights.write(np_array_A1D_string)\n",
    "    \n",
    "\n",
    "    # # Calculate and save the average error\n",
    "    # avg_error = tot_error / num_batches\n",
    "    # with open('S1_Error(Perturbed_high).txt', 'a') as S1_output:\n",
    "    #     S1_output.write(f\"{avg_error}\\n\")\n",
    "\n",
    "    return state  # Reset tot_error for the next outer loop iteration\n",
    "\n",
    "# Initialize the loop variables, \n",
    "# also adding the batch size and the length of the halton sequence\n",
    "len_halton = len(flat_halton_sequence)\n",
    "batch_size = 100\n",
    "batch_halton = flat_halton_sequence[0 : batch_size]\n",
    "\n",
    "print(A1D)\n",
    "state = (A1D, opt_state, 0.0, batch_size, len_halton, flat_halton_sequence, batch_halton)\n",
    "\n",
    "# Perform the optimization loop using lax.fori_loop\n",
    "state = jax.lax.fori_loop(0, 10, outer_loop_body, state)\n",
    "A1D, opt_state, _,_,_,_,_ = state\n",
    "\n",
    "print(A1D)\n",
    "\n",
    "\"\"\" \n",
    "Ravi read this : Storing of each and every error wont be possible in jax loop\n",
    "i will have to create a list and store all the answers there \n",
    "it will only be accessiable after the loop is completed, which wont be a good idea ,\n",
    "\n",
    "as right now i am also check the output as the program runs \n",
    "\n",
    "so for now let the optimization use the normal loop and later towards the end i can\n",
    "change it to jax fori loop.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Save the latest A1D values to the weights file\n",
    "# np_array_A1D = jnp.array(A1D)\n",
    "# np_array_A1D_string = ' '.join(map(str, np_array_A1D))\n",
    "# with open('S1_Final_weights(Perturbed_high).txt', 'w') as S1_weights:\n",
    "#     S1_weights.write(np_array_A1D_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "# Simple jax fori loop program :\n",
    "\n",
    "def test_Fori(i, state):\n",
    "    s = state\n",
    "    s = s + s\n",
    "    state = s\n",
    "    return state\n",
    "\n",
    "sum1 = 1\n",
    "state = (sum1)\n",
    "final_state = jax.lax.fori_loop(0,11, test_Fori, state)\n",
    "print(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "Traced<ShapedArray(int64[5])>with<DynamicJaxprTrace(level=1/0)> /n\n",
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Initialize two JAX arrays\n",
    "array1 = jnp.array([1, 2, 3, 4, 5])\n",
    "array2 = jnp.zeros_like(array1)\n",
    "\n",
    "print(array2)\n",
    "\n",
    "def print_func(n):\n",
    "    print(n, \"/n\")\n",
    "# Define the loop body function\n",
    "def loop_body(i, array2):\n",
    "    print_func(array2) # this wont work\n",
    "    return array2.at[i].set(array1[i])\n",
    "\n",
    "# Perform the loop using lax.fori_loop\n",
    "array2 = jax.lax.fori_loop(0, len(array1), loop_body, array2)\n",
    "\n",
    "print(array2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
